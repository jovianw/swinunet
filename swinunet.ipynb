{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19096,
     "status": "ok",
     "timestamp": 1708324775374,
     "user": {
      "displayName": "NGKD",
      "userId": "00773922209354261622"
     },
     "user_tz": 360
    },
    "id": "4MdtSq298luC",
    "outputId": "50475c3f-52cf-4fbb-e357-eb783668a347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 2)) (0.16.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 4)) (4.66.2)\n",
      "Requirement already satisfied: ml-collections in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 7)) (0.1.1)\n",
      "Requirement already satisfied: medpy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: SimpleITK in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 9)) (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 10)) (1.10.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 11)) (3.8.0)\n",
      "Requirement already satisfied: timm in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 12)) (0.9.16)\n",
      "Requirement already satisfied: einops in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2023.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (10.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (6.0.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: contextlib2 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (21.6.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from timm->-r requirements.txt (line 12)) (0.4.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from timm->-r requirements.txt (line 12)) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from huggingface_hub->timm->-r requirements.txt (line 12)) (23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xV37eWL2GG0l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from skimage.transform import resize\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import DiceLoss\n",
    "from torchvision import transforms\n",
    "from utils import test_single_volume\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/swin_tiny_patch4_window7_224_lite.yaml\n"
     ]
    }
   ],
   "source": [
    "from config import get_config\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "args.cfg = \"configs/swin_tiny_patch4_window7_224_lite.yaml\"\n",
    "args.batch_size = 12\n",
    "args.cache_mode = 'no'\n",
    "\n",
    "config = get_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = [f.split('.')[0] for f in os.listdir(image_dir) if f.endswith('.nii.gz')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.filenames[idx] + '.nii.gz')\n",
    "        label_path_zip = os.path.join(self.label_dir, self.filenames[idx] + '.nii.gz.zip')\n",
    "        \n",
    "        # Load image\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        # Extract and load label\n",
    "        with zipfile.ZipFile(label_path_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall('temp_labels')\n",
    "        label_path = os.path.join('temp_labels', self.filenames[idx] + '.nii.gz')\n",
    "        label = nib.load(label_path).get_fdata()\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        # Clean up extracted files to save space\n",
    "        os.remove(label_path)\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample['case_name'] = self.filenames[idx].strip('\\n')\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train set is: 744\n"
     ]
    }
   ],
   "source": [
    "image_dir = 'datasets/ULS23_Radboudumc_Bone/images'\n",
    "label_dir = 'datasets/ULS23_Radboudumc_Bone/labels'\n",
    "\n",
    "db_train = NiftiDataset(image_dir, label_dir)\n",
    "print(\"The length of train set is: {}\".format(len(db_train)))\n",
    "trainloader  = DataLoader(db_train, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:2\n",
      "---final upsample expand_first---\n",
      "pretrained_path:./pretrained_ckpt/swin_tiny_patch4_window7_224.pth\n",
      "---start load pretrained modle of swin encoder---\n"
     ]
    }
   ],
   "source": [
    "from networks.vision_transformer import SwinUnet\n",
    "\n",
    "model = SwinUnet(config, img_size=256, num_classes=2).cuda()\n",
    "model.load_from(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinUnet(\n",
       "  (swin_unet): SwinTransformerSys(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        dim=96, input_resolution=(56, 56), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.029)\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(56, 56), dim=96\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=192, input_resolution=(28, 28), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.086)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(28, 28), dim=192\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=384, input_resolution=(14, 14), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.114)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.143)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(14, 14), dim=384\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=768, input_resolution=(7, 7), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(7, 7), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.171)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(7, 7), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.200)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers_up): ModuleList(\n",
       "      (0): PatchExpand(\n",
       "        (expand): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BasicLayer_up(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.114)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.143)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchExpand(\n",
       "          (expand): Linear(in_features=384, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer_up(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.086)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchExpand(\n",
       "          (expand): Linear(in_features=192, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer_up(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.029)\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (concat_back_dim): ModuleList(\n",
       "      (0): Identity()\n",
       "      (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "      (3): Linear(in_features=192, out_features=96, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_up): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    (up): FinalPatchExpand_X4(\n",
       "      (expand): Linear(in_features=96, out_features=1536, bias=False)\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (output): Conv2d(96, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = CrossEntropyLoss()\n",
    "dice_loss = DiceLoss(2)\n",
    "base_lr = 0.05\n",
    "optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "max_epoch = 500\n",
    "max_iterations = max_epoch * len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performance = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_batch, sampled_batch = next(enumerate(trainloader))\n",
    "image_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
    "image_batch, label_batch = image_batch.cuda(), label_batch.cuda()\n",
    "image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bl83iyGHGH08"
   },
   "outputs": [],
   "source": [
    "# Paths to your datasets\n",
    "data_path = '/content/drive/MyDrive/Data/NGKDStorage/images/ULS23_Part1/ULS23/novel_data/ULS23_Radboudumc_Bone/images'\n",
    "annotations_path = '/content/drive/MyDrive/Data/NGKDStorage/annotations/ULS23/novel_data/ULS23_Radboudumc_Bone/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZmXOIkhGJhe"
   },
   "outputs": [],
   "source": [
    "class NiftiDataGenerator(Sequence):\n",
    "    def __init__(self, data_filenames, annotations_filenames, batch_size, data_path, annotations_path, image_size=(256, 256)):\n",
    "        self.data_filenames = data_filenames\n",
    "        self.annotations_filenames = annotations_filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        self.annotations_path = annotations_path\n",
    "        self.image_size = image_size\n",
    "        self.indexes = self._get_indexes()\n",
    "\n",
    "    def _get_indexes(self):\n",
    "        indexes = []\n",
    "        for i, fname in enumerate(self.data_filenames):\n",
    "            nii_img = nib.load(os.path.join(self.data_path, fname))\n",
    "            num_slices = nii_img.shape[2]\n",
    "            for slice_index in range(num_slices):\n",
    "                indexes.append((i, slice_index))\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indexes) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_data = []\n",
    "        batch_annotations = []\n",
    "\n",
    "        for i, slice_index in batch_indexes:\n",
    "            data_filename = self.data_filenames[i]\n",
    "            annotations_filename = self.annotations_filenames[i]\n",
    "\n",
    "            data_nii = nib.load(os.path.join(self.data_path, data_filename))\n",
    "            annotations_nii = nib.load(os.path.join(self.annotations_path, annotations_filename))\n",
    "\n",
    "            data_img = data_nii.get_fdata()[:, :, slice_index]\n",
    "            annotations_img = annotations_nii.get_fdata()[:, :, slice_index]\n",
    "\n",
    "            # Convert grayscale to 3 channels by repeating the slice\n",
    "            data_slice_3ch = np.repeat(data_img[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "            # Resize images (if necessary)\n",
    "            data_resized = resize(data_slice_3ch, self.image_size + (3,), anti_aliasing=True)\n",
    "            annotations_resized = resize(annotations_img, self.image_size, anti_aliasing=True, order=0, preserve_range=True)\n",
    "\n",
    "            # Remove the unwanted dimension safely\n",
    "            if data_resized.shape[-1] == 1:\n",
    "                data_resized = data_resized[..., 0]\n",
    "\n",
    "            batch_data.append(data_resized)\n",
    "            batch_annotations.append(annotations_resized)\n",
    "\n",
    "        return np.array(batch_data), np.array(batch_annotations).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAfwewBTGLUA"
   },
   "outputs": [],
   "source": [
    "# Get the list of filenames for data and annotations\n",
    "data_filenames = [f for f in os.listdir(data_path) if f.endswith('.nii.gz')]\n",
    "annotations_filenames = [f for f in os.listdir(annotations_path) if f.endswith('.nii.gz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEzTImjDGPD5"
   },
   "outputs": [],
   "source": [
    "# Sort the lists to ensure they are aligned\n",
    "data_filenames.sort()\n",
    "annotations_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ogFzagLGQ3U"
   },
   "outputs": [],
   "source": [
    "# Split the filenames into training and validation sets\n",
    "train_data_filenames, val_data_filenames, train_annotations_filenames, val_annotations_filenames = train_test_split(data_filenames, annotations_filenames, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25Arkb3kGaxS"
   },
   "outputs": [],
   "source": [
    "batch_size = 4  # Adjust based on your GPU memory\n",
    "\n",
    "# Create the data generators\n",
    "train_generator = NiftiDataGenerator(\n",
    "    data_filenames=train_data_filenames,\n",
    "    annotations_filenames=train_annotations_filenames,\n",
    "    batch_size=batch_size,\n",
    "    data_path=data_path,\n",
    "    annotations_path=annotations_path\n",
    ")\n",
    "\n",
    "val_generator = NiftiDataGenerator(\n",
    "    data_filenames=val_data_filenames,\n",
    "    annotations_filenames=val_annotations_filenames,\n",
    "    batch_size=batch_size,\n",
    "    data_path=data_path,\n",
    "    annotations_path=annotations_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKXy9y94G17L"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def DeeplabV3Plus(image_size, num_classes):\n",
    "    # Load a pretrained MobileNetV2 model as a base\n",
    "    base_model = MobileNetV2(input_shape=[image_size, image_size, 3], include_top=False)\n",
    "\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "        'block_1_expand_relu',   # 64x64\n",
    "        'block_3_expand_relu',   # 32x32\n",
    "        'block_6_expand_relu',   # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "        'block_16_project',      # 4x4\n",
    "    ]\n",
    "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Create the feature extraction model\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "    down_stack.trainable = False\n",
    "\n",
    "    # Define the upsampling layers\n",
    "    up_stack = [\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 4x4 -> 8x8\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 8x8 -> 16x16\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 16x16 -> 32x32\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 32x32 -> 64x64\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 64x64 -> 128x128\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 128x128 -> 256x256\n",
    "    ]\n",
    "\n",
    "    inputs = tf.keras.Input(shape=[image_size, image_size, 3])\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(x)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = layers.Conv2DTranspose(\n",
    "        num_classes, 3, strides=2,\n",
    "        padding='same', activation='softmax')  # Use strides=2 to match the original image size if needed\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLP1CMGyKsNt"
   },
   "outputs": [],
   "source": [
    "# Define the size of your images and number of classes\n",
    "image_size = 256  # Size of your input images\n",
    "num_classes = 2   # Two classes: background and lesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avU9W_OHMmMG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.cast(y_pred, tf.float32))\n",
    "    union = tf.reduce_sum(tf.cast(y_true, tf.float32)) + tf.reduce_sum(tf.cast(y_pred, tf.float32))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice\n",
    "\n",
    "# Custom callback to print the Dice score after each epoch\n",
    "class PrintDiceScoreCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}: Dice Score is {logs['dice_score']:.4f} for training and {logs['val_dice_score']:.4f} for validation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1984,
     "status": "ok",
     "timestamp": 1708329203507,
     "user": {
      "displayName": "NGKD",
      "userId": "00773922209354261622"
     },
     "user_tz": 360
    },
    "id": "h3jgDl-4Ktpg",
    "outputId": "bf5cb564-b1e7-42eb-a2f1-64799b098a35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " model_26 (Functional)       [(None, 128, 128, 96),       1841984   ['input_28[0][0]']            \n",
      "                              (None, 64, 64, 144),                                                \n",
      "                              (None, 32, 32, 192),                                                \n",
      "                              (None, 16, 16, 576),                                                \n",
      "                              (None, 8, 8, 320)]                                                  \n",
      "                                                                                                  \n",
      " up_sampling2d_78 (UpSampli  (None, 16, 16, 320)          0         ['model_26[0][4]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_52 (Concatenat  (None, 16, 16, 896)          0         ['up_sampling2d_78[0][0]',    \n",
      " e)                                                                  'model_26[0][3]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_79 (UpSampli  (None, 32, 32, 896)          0         ['concatenate_52[0][0]']      \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_53 (Concatenat  (None, 32, 32, 1088)         0         ['up_sampling2d_79[0][0]',    \n",
      " e)                                                                  'model_26[0][2]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_80 (UpSampli  (None, 64, 64, 1088)         0         ['concatenate_53[0][0]']      \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_54 (Concatenat  (None, 64, 64, 1232)         0         ['up_sampling2d_80[0][0]',    \n",
      " e)                                                                  'model_26[0][1]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_81 (UpSampli  (None, 128, 128, 1232)       0         ['concatenate_54[0][0]']      \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_55 (Concatenat  (None, 128, 128, 1328)       0         ['up_sampling2d_81[0][0]',    \n",
      " e)                                                                  'model_26[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_13 (Conv2  (None, 256, 256, 2)          23906     ['concatenate_55[0][0]']      \n",
      " DTranspose)                                                                                      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1865890 (7.12 MB)\n",
      "Trainable params: 23906 (93.38 KB)\n",
      "Non-trainable params: 1841984 (7.03 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = DeeplabV3Plus(image_size, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[dice_score])\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuHvMbXyMpI6"
   },
   "outputs": [],
   "source": [
    "# Define, compile, and train the model\n",
    "def train_model():\n",
    "    model = None  # Declare model variable first\n",
    "    # Check if a model already exists\n",
    "    model_path = './my_deeplab_model.h5'\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing model.\")\n",
    "        model = tf.keras.models.load_model(model_path, custom_objects={'dice_score': dice_score})\n",
    "    else:\n",
    "        print(\"Creating a new model.\")\n",
    "        model = DeeplabV3Plus(image_size, num_classes)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=[dice_score])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        validation_steps=len(val_generator),\n",
    "        callbacks=[PrintDiceScoreCallback()]\n",
    "    )\n",
    "\n",
    "    # Save the model after training\n",
    "    model.save(model_path)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzdiWDBXMe_9",
    "outputId": "15e262e3-20ca-4fe8-c6df-adb4e69403ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new model.\n",
      " 2264/19040 [==>...........................] - ETA: 10:08:20 - loss: 0.0110 - dice_score: 0.8225"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = train_model()\n",
    "\n",
    "# Plot the Dice score\n",
    "plt.plot(history.history['dice_score'], label='Training Dice Score')\n",
    "plt.plot(history.history['val_dice_score'], label='Validation Dice Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
