{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19096,
     "status": "ok",
     "timestamp": 1708324775374,
     "user": {
      "displayName": "NGKD",
      "userId": "00773922209354261622"
     },
     "user_tz": 360
    },
    "id": "4MdtSq298luC",
    "outputId": "50475c3f-52cf-4fbb-e357-eb783668a347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 2)) (0.16.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 4)) (4.66.2)\n",
      "Requirement already satisfied: ml-collections in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 7)) (0.1.1)\n",
      "Requirement already satisfied: medpy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: SimpleITK in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 9)) (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 10)) (1.10.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 11)) (3.8.0)\n",
      "Requirement already satisfied: timm in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 12)) (0.9.16)\n",
      "Requirement already satisfied: einops in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from -r requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2023.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (10.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (6.0.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: contextlib2 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from ml-collections->-r requirements.txt (line 7)) (21.6.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from timm->-r requirements.txt (line 12)) (0.4.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from timm->-r requirements.txt (line 12)) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from huggingface_hub->timm->-r requirements.txt (line 12)) (23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jovian\\anaconda3\\envs\\python38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xV37eWL2GG0l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from skimage.transform import resize\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import DiceLoss\n",
    "from torchvision import transforms\n",
    "from utils import test_single_volume\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/swin_tiny_patch4_window7_224_lite.yaml\n"
     ]
    }
   ],
   "source": [
    "from config import get_config\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "args.cfg = \"configs/swin_tiny_patch4_window7_224_lite.yaml\"\n",
    "args.batch_size = 12\n",
    "args.cache_mode = 'no'\n",
    "\n",
    "config = get_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = [f.split('.')[0] for f in os.listdir(image_dir) if f.endswith('.nii.gz')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.filenames[idx] + '.nii.gz')\n",
    "        label_path_zip = os.path.join(self.label_dir, self.filenames[idx] + '.nii.gz.zip')\n",
    "        \n",
    "        # Load image\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        # Extract and load label\n",
    "        with zipfile.ZipFile(label_path_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall('temp_labels')\n",
    "        label_path = os.path.join('temp_labels', self.filenames[idx] + '.nii.gz')\n",
    "        label = nib.load(label_path).get_fdata()\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        # Clean up extracted files to save space\n",
    "        os.remove(label_path)\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample['case_name'] = self.filenames[idx].strip('\\n')\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train set is: 744\n"
     ]
    }
   ],
   "source": [
    "image_dir = 'datasets/ULS23_Radboudumc_Bone/images'\n",
    "label_dir = 'datasets/ULS23_Radboudumc_Bone/labels'\n",
    "\n",
    "db_train = NiftiDataset(image_dir, label_dir)\n",
    "print(\"The length of train set is: {}\".format(len(db_train)))\n",
    "trainloader  = DataLoader(db_train, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:2\n",
      "---final upsample expand_first---\n",
      "pretrained_path:./pretrained_ckpt/swin_tiny_patch4_window7_224.pth\n",
      "---start load pretrained modle of swin encoder---\n"
     ]
    }
   ],
   "source": [
    "from networks.vision_transformer import SwinUnet\n",
    "\n",
    "model = SwinUnet(config, img_size=256, num_classes=2).cuda()\n",
    "model.load_from(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinUnet(\n",
       "  (swin_unet): SwinTransformerSys(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        dim=96, input_resolution=(56, 56), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.029)\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(56, 56), dim=96\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=192, input_resolution=(28, 28), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.086)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(28, 28), dim=192\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=384, input_resolution=(14, 14), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.114)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.143)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(14, 14), dim=384\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=768, input_resolution=(7, 7), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(7, 7), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.171)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=(7, 7), num_heads=24\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.200)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers_up): ModuleList(\n",
       "      (0): PatchExpand(\n",
       "        (expand): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BasicLayer_up(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.114)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=(7, 7), num_heads=12\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.143)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchExpand(\n",
       "          (expand): Linear(in_features=384, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer_up(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=(7, 7), num_heads=6\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.086)\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchExpand(\n",
       "          (expand): Linear(in_features=192, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer_up(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=(7, 7), num_heads=3\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.029)\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (concat_back_dim): ModuleList(\n",
       "      (0): Identity()\n",
       "      (1): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "      (3): Linear(in_features=192, out_features=96, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_up): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    (up): FinalPatchExpand_X4(\n",
       "      (expand): Linear(in_features=96, out_features=1536, bias=False)\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (output): Conv2d(96, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = CrossEntropyLoss()\n",
    "dice_loss = DiceLoss(2)\n",
    "base_lr = 0.05\n",
    "optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "max_epoch = 500\n",
    "max_iterations = max_epoch * len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performance = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[  -87.],\n",
       "           [  -78.],\n",
       "           [  -91.],\n",
       "           ...,\n",
       "           [  -96.],\n",
       "           [ -127.],\n",
       "           [ -118.]],\n",
       "\n",
       "          [[  -99.],\n",
       "           [ -109.],\n",
       "           [ -104.],\n",
       "           ...,\n",
       "           [ -105.],\n",
       "           [ -102.],\n",
       "           [  -85.]],\n",
       "\n",
       "          [[ -138.],\n",
       "           [ -139.],\n",
       "           [ -129.],\n",
       "           ...,\n",
       "           [ -126.],\n",
       "           [ -118.],\n",
       "           [ -111.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ -107.],\n",
       "           [ -120.],\n",
       "           [ -113.],\n",
       "           ...,\n",
       "           [   33.],\n",
       "           [   38.],\n",
       "           [   24.]],\n",
       "\n",
       "          [[ -118.],\n",
       "           [ -124.],\n",
       "           [ -117.],\n",
       "           ...,\n",
       "           [   27.],\n",
       "           [   53.],\n",
       "           [   49.]],\n",
       "\n",
       "          [[ -122.],\n",
       "           [ -112.],\n",
       "           [  -98.],\n",
       "           ...,\n",
       "           [   51.],\n",
       "           [   67.],\n",
       "           [   65.]]],\n",
       "\n",
       "\n",
       "         [[[  -87.],\n",
       "           [  -79.],\n",
       "           [  -93.],\n",
       "           ...,\n",
       "           [ -119.],\n",
       "           [ -112.],\n",
       "           [  -96.]],\n",
       "\n",
       "          [[  -80.],\n",
       "           [ -103.],\n",
       "           [ -114.],\n",
       "           ...,\n",
       "           [ -105.],\n",
       "           [  -65.],\n",
       "           [  -53.]],\n",
       "\n",
       "          [[ -102.],\n",
       "           [ -106.],\n",
       "           [ -112.],\n",
       "           ...,\n",
       "           [ -102.],\n",
       "           [ -101.],\n",
       "           [  -98.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ -110.],\n",
       "           [ -117.],\n",
       "           [ -106.],\n",
       "           ...,\n",
       "           [   15.],\n",
       "           [   39.],\n",
       "           [   76.]],\n",
       "\n",
       "          [[  -96.],\n",
       "           [  -99.],\n",
       "           [ -110.],\n",
       "           ...,\n",
       "           [   12.],\n",
       "           [   15.],\n",
       "           [   70.]],\n",
       "\n",
       "          [[ -115.],\n",
       "           [ -104.],\n",
       "           [ -114.],\n",
       "           ...,\n",
       "           [   39.],\n",
       "           [   73.],\n",
       "           [  116.]]],\n",
       "\n",
       "\n",
       "         [[[ -130.],\n",
       "           [ -103.],\n",
       "           [  -96.],\n",
       "           ...,\n",
       "           [  -74.],\n",
       "           [  -63.],\n",
       "           [  -87.]],\n",
       "\n",
       "          [[  -67.],\n",
       "           [  -71.],\n",
       "           [  -96.],\n",
       "           ...,\n",
       "           [  -85.],\n",
       "           [  -35.],\n",
       "           [  -60.]],\n",
       "\n",
       "          [[ -102.],\n",
       "           [ -107.],\n",
       "           [ -125.],\n",
       "           ...,\n",
       "           [ -121.],\n",
       "           [ -105.],\n",
       "           [ -108.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ -107.],\n",
       "           [ -127.],\n",
       "           [ -121.],\n",
       "           ...,\n",
       "           [   37.],\n",
       "           [   37.],\n",
       "           [   43.]],\n",
       "\n",
       "          [[  -99.],\n",
       "           [ -104.],\n",
       "           [ -101.],\n",
       "           ...,\n",
       "           [   36.],\n",
       "           [   23.],\n",
       "           [   78.]],\n",
       "\n",
       "          [[  -98.],\n",
       "           [ -107.],\n",
       "           [ -106.],\n",
       "           ...,\n",
       "           [   57.],\n",
       "           [  106.],\n",
       "           [  156.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          [[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          [[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ -998.],\n",
       "           [ -989.],\n",
       "           [ -992.],\n",
       "           ...,\n",
       "           [ -980.],\n",
       "           [ -970.],\n",
       "           [ -987.]],\n",
       "\n",
       "          [[ -989.],\n",
       "           [-1000.],\n",
       "           [-1010.],\n",
       "           ...,\n",
       "           [ -977.],\n",
       "           [ -970.],\n",
       "           [-1006.]],\n",
       "\n",
       "          [[-1002.],\n",
       "           [-1010.],\n",
       "           [-1022.],\n",
       "           ...,\n",
       "           [-1003.],\n",
       "           [ -991.],\n",
       "           [-1004.]]],\n",
       "\n",
       "\n",
       "         [[[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          [[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          [[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-1006.],\n",
       "           [-1000.],\n",
       "           [ -991.],\n",
       "           ...,\n",
       "           [ -982.],\n",
       "           [ -959.],\n",
       "           [ -963.]],\n",
       "\n",
       "          [[ -990.],\n",
       "           [ -984.],\n",
       "           [ -980.],\n",
       "           ...,\n",
       "           [ -960.],\n",
       "           [ -950.],\n",
       "           [ -984.]],\n",
       "\n",
       "          [[-1002.],\n",
       "           [ -996.],\n",
       "           [-1017.],\n",
       "           ...,\n",
       "           [ -979.],\n",
       "           [ -976.],\n",
       "           [ -996.]]],\n",
       "\n",
       "\n",
       "         [[[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          [[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          [[-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           ...,\n",
       "           [-2048.],\n",
       "           [-2048.],\n",
       "           [-2048.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[-1001.],\n",
       "           [ -996.],\n",
       "           [ -975.],\n",
       "           ...,\n",
       "           [-1013.],\n",
       "           [ -996.],\n",
       "           [ -971.]],\n",
       "\n",
       "          [[ -992.],\n",
       "           [ -996.],\n",
       "           [ -978.],\n",
       "           ...,\n",
       "           [ -976.],\n",
       "           [ -957.],\n",
       "           [ -980.]],\n",
       "\n",
       "          [[ -996.],\n",
       "           [ -995.],\n",
       "           [-1007.],\n",
       "           ...,\n",
       "           [ -977.],\n",
       "           [ -958.],\n",
       "           [ -991.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[   91.],\n",
       "           [   81.],\n",
       "           [   67.],\n",
       "           ...,\n",
       "           [   70.],\n",
       "           [   80.],\n",
       "           [   62.]],\n",
       "\n",
       "          [[   71.],\n",
       "           [   55.],\n",
       "           [   74.],\n",
       "           ...,\n",
       "           [   97.],\n",
       "           [   90.],\n",
       "           [   48.]],\n",
       "\n",
       "          [[   77.],\n",
       "           [   59.],\n",
       "           [   77.],\n",
       "           ...,\n",
       "           [   80.],\n",
       "           [   83.],\n",
       "           [   63.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1130.],\n",
       "           [ 1124.],\n",
       "           [ 1094.],\n",
       "           ...,\n",
       "           [ 1092.],\n",
       "           [ 1108.],\n",
       "           [ 1107.]],\n",
       "\n",
       "          [[ 1122.],\n",
       "           [ 1111.],\n",
       "           [ 1090.],\n",
       "           ...,\n",
       "           [ 1130.],\n",
       "           [ 1129.],\n",
       "           [ 1117.]],\n",
       "\n",
       "          [[ 1094.],\n",
       "           [ 1095.],\n",
       "           [ 1083.],\n",
       "           ...,\n",
       "           [ 1163.],\n",
       "           [ 1189.],\n",
       "           [ 1143.]]],\n",
       "\n",
       "\n",
       "         [[[   83.],\n",
       "           [   91.],\n",
       "           [   74.],\n",
       "           ...,\n",
       "           [   50.],\n",
       "           [   44.],\n",
       "           [   71.]],\n",
       "\n",
       "          [[   74.],\n",
       "           [   78.],\n",
       "           [   79.],\n",
       "           ...,\n",
       "           [   82.],\n",
       "           [   73.],\n",
       "           [   52.]],\n",
       "\n",
       "          [[   82.],\n",
       "           [   72.],\n",
       "           [   78.],\n",
       "           ...,\n",
       "           [   91.],\n",
       "           [   99.],\n",
       "           [   68.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1112.],\n",
       "           [ 1092.],\n",
       "           [ 1097.],\n",
       "           ...,\n",
       "           [ 1093.],\n",
       "           [ 1067.],\n",
       "           [ 1114.]],\n",
       "\n",
       "          [[ 1124.],\n",
       "           [ 1096.],\n",
       "           [ 1106.],\n",
       "           ...,\n",
       "           [ 1111.],\n",
       "           [ 1077.],\n",
       "           [ 1106.]],\n",
       "\n",
       "          [[ 1118.],\n",
       "           [ 1088.],\n",
       "           [ 1096.],\n",
       "           ...,\n",
       "           [ 1115.],\n",
       "           [ 1119.],\n",
       "           [ 1116.]]],\n",
       "\n",
       "\n",
       "         [[[   72.],\n",
       "           [   89.],\n",
       "           [   77.],\n",
       "           ...,\n",
       "           [   60.],\n",
       "           [   40.],\n",
       "           [   82.]],\n",
       "\n",
       "          [[   76.],\n",
       "           [   79.],\n",
       "           [   63.],\n",
       "           ...,\n",
       "           [   77.],\n",
       "           [   54.],\n",
       "           [   79.]],\n",
       "\n",
       "          [[   88.],\n",
       "           [   88.],\n",
       "           [   74.],\n",
       "           ...,\n",
       "           [   90.],\n",
       "           [   61.],\n",
       "           [   61.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1100.],\n",
       "           [ 1079.],\n",
       "           [ 1077.],\n",
       "           ...,\n",
       "           [ 1110.],\n",
       "           [ 1093.],\n",
       "           [ 1127.]],\n",
       "\n",
       "          [[ 1102.],\n",
       "           [ 1094.],\n",
       "           [ 1107.],\n",
       "           ...,\n",
       "           [ 1123.],\n",
       "           [ 1102.],\n",
       "           [ 1104.]],\n",
       "\n",
       "          [[ 1112.],\n",
       "           [ 1095.],\n",
       "           [ 1128.],\n",
       "           ...,\n",
       "           [ 1088.],\n",
       "           [ 1086.],\n",
       "           [ 1094.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[  161.],\n",
       "           [  212.],\n",
       "           [  167.],\n",
       "           ...,\n",
       "           [   98.],\n",
       "           [  123.],\n",
       "           [   89.]],\n",
       "\n",
       "          [[  274.],\n",
       "           [  317.],\n",
       "           [  254.],\n",
       "           ...,\n",
       "           [  173.],\n",
       "           [  205.],\n",
       "           [  174.]],\n",
       "\n",
       "          [[  315.],\n",
       "           [  326.],\n",
       "           [  307.],\n",
       "           ...,\n",
       "           [  264.],\n",
       "           [  287.],\n",
       "           [  242.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1339.],\n",
       "           [ 1320.],\n",
       "           [ 1380.],\n",
       "           ...,\n",
       "           [ 1199.],\n",
       "           [ 1201.],\n",
       "           [ 1195.]],\n",
       "\n",
       "          [[ 1311.],\n",
       "           [ 1330.],\n",
       "           [ 1446.],\n",
       "           ...,\n",
       "           [ 1172.],\n",
       "           [ 1178.],\n",
       "           [ 1164.]],\n",
       "\n",
       "          [[ 1392.],\n",
       "           [ 1388.],\n",
       "           [ 1441.],\n",
       "           ...,\n",
       "           [ 1175.],\n",
       "           [ 1158.],\n",
       "           [ 1118.]]],\n",
       "\n",
       "\n",
       "         [[[  157.],\n",
       "           [  188.],\n",
       "           [  129.],\n",
       "           ...,\n",
       "           [   74.],\n",
       "           [   80.],\n",
       "           [  101.]],\n",
       "\n",
       "          [[  252.],\n",
       "           [  279.],\n",
       "           [  228.],\n",
       "           ...,\n",
       "           [  133.],\n",
       "           [  152.],\n",
       "           [  143.]],\n",
       "\n",
       "          [[  299.],\n",
       "           [  300.],\n",
       "           [  286.],\n",
       "           ...,\n",
       "           [  235.],\n",
       "           [  246.],\n",
       "           [  204.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1348.],\n",
       "           [ 1334.],\n",
       "           [ 1400.],\n",
       "           ...,\n",
       "           [ 1152.],\n",
       "           [ 1176.],\n",
       "           [ 1161.]],\n",
       "\n",
       "          [[ 1326.],\n",
       "           [ 1318.],\n",
       "           [ 1429.],\n",
       "           ...,\n",
       "           [ 1167.],\n",
       "           [ 1199.],\n",
       "           [ 1166.]],\n",
       "\n",
       "          [[ 1350.],\n",
       "           [ 1333.],\n",
       "           [ 1378.],\n",
       "           ...,\n",
       "           [ 1161.],\n",
       "           [ 1179.],\n",
       "           [ 1130.]]],\n",
       "\n",
       "\n",
       "         [[[  117.],\n",
       "           [  135.],\n",
       "           [  113.],\n",
       "           ...,\n",
       "           [   79.],\n",
       "           [   56.],\n",
       "           [   78.]],\n",
       "\n",
       "          [[  228.],\n",
       "           [  241.],\n",
       "           [  218.],\n",
       "           ...,\n",
       "           [  131.],\n",
       "           [  103.],\n",
       "           [   94.]],\n",
       "\n",
       "          [[  278.],\n",
       "           [  289.],\n",
       "           [  267.],\n",
       "           ...,\n",
       "           [  221.],\n",
       "           [  200.],\n",
       "           [  188.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1396.],\n",
       "           [ 1422.],\n",
       "           [ 1436.],\n",
       "           ...,\n",
       "           [ 1129.],\n",
       "           [ 1142.],\n",
       "           [ 1127.]],\n",
       "\n",
       "          [[ 1380.],\n",
       "           [ 1393.],\n",
       "           [ 1456.],\n",
       "           ...,\n",
       "           [ 1150.],\n",
       "           [ 1181.],\n",
       "           [ 1146.]],\n",
       "\n",
       "          [[ 1371.],\n",
       "           [ 1364.],\n",
       "           [ 1388.],\n",
       "           ...,\n",
       "           [ 1128.],\n",
       "           [ 1160.],\n",
       "           [ 1131.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[  119.],\n",
       "           [  124.],\n",
       "           [  136.],\n",
       "           ...,\n",
       "           [  119.],\n",
       "           [  125.],\n",
       "           [  137.]],\n",
       "\n",
       "          [[  123.],\n",
       "           [  132.],\n",
       "           [  132.],\n",
       "           ...,\n",
       "           [  115.],\n",
       "           [  136.],\n",
       "           [  123.]],\n",
       "\n",
       "          [[  134.],\n",
       "           [  131.],\n",
       "           [  129.],\n",
       "           ...,\n",
       "           [  117.],\n",
       "           [  131.],\n",
       "           [  109.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1035.],\n",
       "           [ 1012.],\n",
       "           [ 1023.],\n",
       "           ...,\n",
       "           [  279.],\n",
       "           [  196.],\n",
       "           [  167.]],\n",
       "\n",
       "          [[ 1052.],\n",
       "           [ 1031.],\n",
       "           [ 1009.],\n",
       "           ...,\n",
       "           [  152.],\n",
       "           [  142.],\n",
       "           [  120.]],\n",
       "\n",
       "          [[ 1014.],\n",
       "           [  997.],\n",
       "           [ 1005.],\n",
       "           ...,\n",
       "           [  119.],\n",
       "           [  128.],\n",
       "           [  113.]]],\n",
       "\n",
       "\n",
       "         [[[  110.],\n",
       "           [   99.],\n",
       "           [  132.],\n",
       "           ...,\n",
       "           [  122.],\n",
       "           [  107.],\n",
       "           [  112.]],\n",
       "\n",
       "          [[  109.],\n",
       "           [  109.],\n",
       "           [  133.],\n",
       "           ...,\n",
       "           [  127.],\n",
       "           [  127.],\n",
       "           [  125.]],\n",
       "\n",
       "          [[  112.],\n",
       "           [  134.],\n",
       "           [  157.],\n",
       "           ...,\n",
       "           [  131.],\n",
       "           [  144.],\n",
       "           [  129.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1060.],\n",
       "           [ 1018.],\n",
       "           [ 1027.],\n",
       "           ...,\n",
       "           [  206.],\n",
       "           [  117.],\n",
       "           [  109.]],\n",
       "\n",
       "          [[ 1015.],\n",
       "           [ 1009.],\n",
       "           [ 1002.],\n",
       "           ...,\n",
       "           [  126.],\n",
       "           [   98.],\n",
       "           [  110.]],\n",
       "\n",
       "          [[  994.],\n",
       "           [  993.],\n",
       "           [  983.],\n",
       "           ...,\n",
       "           [  111.],\n",
       "           [  108.],\n",
       "           [  107.]]],\n",
       "\n",
       "\n",
       "         [[[  136.],\n",
       "           [  106.],\n",
       "           [  113.],\n",
       "           ...,\n",
       "           [  149.],\n",
       "           [  124.],\n",
       "           [  112.]],\n",
       "\n",
       "          [[  127.],\n",
       "           [   96.],\n",
       "           [  108.],\n",
       "           ...,\n",
       "           [  141.],\n",
       "           [  113.],\n",
       "           [  106.]],\n",
       "\n",
       "          [[  122.],\n",
       "           [  105.],\n",
       "           [  113.],\n",
       "           ...,\n",
       "           [  133.],\n",
       "           [  133.],\n",
       "           [  117.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1077.],\n",
       "           [ 1065.],\n",
       "           [ 1056.],\n",
       "           ...,\n",
       "           [  238.],\n",
       "           [  118.],\n",
       "           [   88.]],\n",
       "\n",
       "          [[ 1007.],\n",
       "           [ 1054.],\n",
       "           [ 1050.],\n",
       "           ...,\n",
       "           [  156.],\n",
       "           [  107.],\n",
       "           [   96.]],\n",
       "\n",
       "          [[ 1026.],\n",
       "           [ 1051.],\n",
       "           [ 1019.],\n",
       "           ...,\n",
       "           [  110.],\n",
       "           [  110.],\n",
       "           [  117.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[  119.],\n",
       "           [  116.],\n",
       "           [  119.],\n",
       "           ...,\n",
       "           [  127.],\n",
       "           [  123.],\n",
       "           [  128.]],\n",
       "\n",
       "          [[  118.],\n",
       "           [  109.],\n",
       "           [  103.],\n",
       "           ...,\n",
       "           [   94.],\n",
       "           [   83.],\n",
       "           [  109.]],\n",
       "\n",
       "          [[  109.],\n",
       "           [  117.],\n",
       "           [  109.],\n",
       "           ...,\n",
       "           [   75.],\n",
       "           [   68.],\n",
       "           [   95.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1056.],\n",
       "           [ 1070.],\n",
       "           [ 1089.],\n",
       "           ...,\n",
       "           [ 1071.],\n",
       "           [ 1058.],\n",
       "           [ 1013.]],\n",
       "\n",
       "          [[ 1053.],\n",
       "           [ 1064.],\n",
       "           [ 1062.],\n",
       "           ...,\n",
       "           [ 1065.],\n",
       "           [ 1051.],\n",
       "           [ 1027.]],\n",
       "\n",
       "          [[ 1043.],\n",
       "           [ 1057.],\n",
       "           [ 1084.],\n",
       "           ...,\n",
       "           [ 1102.],\n",
       "           [ 1095.],\n",
       "           [ 1115.]]],\n",
       "\n",
       "\n",
       "         [[[  126.],\n",
       "           [  128.],\n",
       "           [  117.],\n",
       "           ...,\n",
       "           [  125.],\n",
       "           [  119.],\n",
       "           [  132.]],\n",
       "\n",
       "          [[  122.],\n",
       "           [  122.],\n",
       "           [   96.],\n",
       "           ...,\n",
       "           [  122.],\n",
       "           [  114.],\n",
       "           [  130.]],\n",
       "\n",
       "          [[  105.],\n",
       "           [   96.],\n",
       "           [   83.],\n",
       "           ...,\n",
       "           [  108.],\n",
       "           [  114.],\n",
       "           [  114.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1093.],\n",
       "           [ 1109.],\n",
       "           [ 1094.],\n",
       "           ...,\n",
       "           [ 1073.],\n",
       "           [ 1048.],\n",
       "           [ 1019.]],\n",
       "\n",
       "          [[ 1071.],\n",
       "           [ 1060.],\n",
       "           [ 1053.],\n",
       "           ...,\n",
       "           [ 1039.],\n",
       "           [ 1041.],\n",
       "           [ 1027.]],\n",
       "\n",
       "          [[ 1076.],\n",
       "           [ 1059.],\n",
       "           [ 1075.],\n",
       "           ...,\n",
       "           [ 1048.],\n",
       "           [ 1081.],\n",
       "           [ 1103.]]],\n",
       "\n",
       "\n",
       "         [[[  153.],\n",
       "           [  165.],\n",
       "           [  121.],\n",
       "           ...,\n",
       "           [  122.],\n",
       "           [  128.],\n",
       "           [  127.]],\n",
       "\n",
       "          [[  130.],\n",
       "           [  116.],\n",
       "           [  104.],\n",
       "           ...,\n",
       "           [  125.],\n",
       "           [  118.],\n",
       "           [  114.]],\n",
       "\n",
       "          [[  120.],\n",
       "           [   90.],\n",
       "           [  102.],\n",
       "           ...,\n",
       "           [  114.],\n",
       "           [  111.],\n",
       "           [  106.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1079.],\n",
       "           [ 1095.],\n",
       "           [ 1101.],\n",
       "           ...,\n",
       "           [ 1070.],\n",
       "           [ 1062.],\n",
       "           [ 1076.]],\n",
       "\n",
       "          [[ 1044.],\n",
       "           [ 1062.],\n",
       "           [ 1074.],\n",
       "           ...,\n",
       "           [ 1040.],\n",
       "           [ 1053.],\n",
       "           [ 1048.]],\n",
       "\n",
       "          [[ 1028.],\n",
       "           [ 1034.],\n",
       "           [ 1053.],\n",
       "           ...,\n",
       "           [ 1020.],\n",
       "           [ 1029.],\n",
       "           [ 1058.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]]],\n",
       "\n",
       "\n",
       "         [[[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]]],\n",
       "\n",
       "\n",
       "         [[[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]],\n",
       "\n",
       "          [[    0.],\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           ...,\n",
       "           [    0.],\n",
       "           [    0.],\n",
       "           [    0.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[  965.],\n",
       "           [  953.],\n",
       "           [  937.],\n",
       "           ...,\n",
       "           [  578.],\n",
       "           [  479.],\n",
       "           [  424.]],\n",
       "\n",
       "          [[  947.],\n",
       "           [  923.],\n",
       "           [  902.],\n",
       "           ...,\n",
       "           [  892.],\n",
       "           [  780.],\n",
       "           [  698.]],\n",
       "\n",
       "          [[  928.],\n",
       "           [  932.],\n",
       "           [  953.],\n",
       "           ...,\n",
       "           [ 1080.],\n",
       "           [ 1028.],\n",
       "           [ 1013.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[  985.],\n",
       "           [  985.],\n",
       "           [  898.],\n",
       "           ...,\n",
       "           [  100.],\n",
       "           [  104.],\n",
       "           [  104.]],\n",
       "\n",
       "          [[ 1039.],\n",
       "           [ 1044.],\n",
       "           [  942.],\n",
       "           ...,\n",
       "           [   92.],\n",
       "           [   83.],\n",
       "           [  103.]],\n",
       "\n",
       "          [[ 1071.],\n",
       "           [ 1092.],\n",
       "           [  975.],\n",
       "           ...,\n",
       "           [   74.],\n",
       "           [   70.],\n",
       "           [   95.]]],\n",
       "\n",
       "\n",
       "         [[[  946.],\n",
       "           [  945.],\n",
       "           [  926.],\n",
       "           ...,\n",
       "           [  605.],\n",
       "           [  519.],\n",
       "           [  438.]],\n",
       "\n",
       "          [[  947.],\n",
       "           [  945.],\n",
       "           [  907.],\n",
       "           ...,\n",
       "           [  939.],\n",
       "           [  851.],\n",
       "           [  737.]],\n",
       "\n",
       "          [[  947.],\n",
       "           [  969.],\n",
       "           [  958.],\n",
       "           ...,\n",
       "           [ 1107.],\n",
       "           [ 1077.],\n",
       "           [ 1039.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1011.],\n",
       "           [ 1033.],\n",
       "           [ 1051.],\n",
       "           ...,\n",
       "           [  102.],\n",
       "           [   97.],\n",
       "           [   89.]],\n",
       "\n",
       "          [[ 1035.],\n",
       "           [ 1085.],\n",
       "           [ 1111.],\n",
       "           ...,\n",
       "           [  106.],\n",
       "           [   82.],\n",
       "           [   89.]],\n",
       "\n",
       "          [[ 1079.],\n",
       "           [ 1139.],\n",
       "           [ 1134.],\n",
       "           ...,\n",
       "           [   89.],\n",
       "           [   81.],\n",
       "           [  104.]]],\n",
       "\n",
       "\n",
       "         [[[  920.],\n",
       "           [  935.],\n",
       "           [  925.],\n",
       "           ...,\n",
       "           [  628.],\n",
       "           [  560.],\n",
       "           [  484.]],\n",
       "\n",
       "          [[  950.],\n",
       "           [  963.],\n",
       "           [  928.],\n",
       "           ...,\n",
       "           [  960.],\n",
       "           [  892.],\n",
       "           [  780.]],\n",
       "\n",
       "          [[  966.],\n",
       "           [  985.],\n",
       "           [  957.],\n",
       "           ...,\n",
       "           [ 1113.],\n",
       "           [ 1095.],\n",
       "           [ 1040.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1031.],\n",
       "           [ 1086.],\n",
       "           [ 1156.],\n",
       "           ...,\n",
       "           [  103.],\n",
       "           [   95.],\n",
       "           [  100.]],\n",
       "\n",
       "          [[ 1026.],\n",
       "           [ 1148.],\n",
       "           [ 1226.],\n",
       "           ...,\n",
       "           [  105.],\n",
       "           [   75.],\n",
       "           [   78.]],\n",
       "\n",
       "          [[ 1108.],\n",
       "           [ 1203.],\n",
       "           [ 1200.],\n",
       "           ...,\n",
       "           [   95.],\n",
       "           [   87.],\n",
       "           [   99.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[   59.],\n",
       "           [   63.],\n",
       "           [   60.],\n",
       "           ...,\n",
       "           [   76.],\n",
       "           [   62.],\n",
       "           [   68.]],\n",
       "\n",
       "          [[   62.],\n",
       "           [   65.],\n",
       "           [   75.],\n",
       "           ...,\n",
       "           [   71.],\n",
       "           [   65.],\n",
       "           [   89.]],\n",
       "\n",
       "          [[   65.],\n",
       "           [   65.],\n",
       "           [   77.],\n",
       "           ...,\n",
       "           [   84.],\n",
       "           [   81.],\n",
       "           [   91.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[  983.],\n",
       "           [ 1022.],\n",
       "           [ 1038.],\n",
       "           ...,\n",
       "           [  920.],\n",
       "           [  892.],\n",
       "           [  919.]],\n",
       "\n",
       "          [[  991.],\n",
       "           [ 1034.],\n",
       "           [ 1019.],\n",
       "           ...,\n",
       "           [  928.],\n",
       "           [  909.],\n",
       "           [  948.]],\n",
       "\n",
       "          [[ 1001.],\n",
       "           [ 1045.],\n",
       "           [  994.],\n",
       "           ...,\n",
       "           [  896.],\n",
       "           [  888.],\n",
       "           [  948.]]],\n",
       "\n",
       "\n",
       "         [[[   56.],\n",
       "           [   40.],\n",
       "           [   48.],\n",
       "           ...,\n",
       "           [   74.],\n",
       "           [   63.],\n",
       "           [   63.]],\n",
       "\n",
       "          [[   53.],\n",
       "           [   49.],\n",
       "           [   59.],\n",
       "           ...,\n",
       "           [   66.],\n",
       "           [   58.],\n",
       "           [   75.]],\n",
       "\n",
       "          [[   70.],\n",
       "           [   60.],\n",
       "           [   73.],\n",
       "           ...,\n",
       "           [   74.],\n",
       "           [   71.],\n",
       "           [   87.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[  928.],\n",
       "           [  972.],\n",
       "           [ 1003.],\n",
       "           ...,\n",
       "           [  885.],\n",
       "           [  886.],\n",
       "           [  925.]],\n",
       "\n",
       "          [[  956.],\n",
       "           [  989.],\n",
       "           [  974.],\n",
       "           ...,\n",
       "           [  907.],\n",
       "           [  920.],\n",
       "           [  949.]],\n",
       "\n",
       "          [[  970.],\n",
       "           [  990.],\n",
       "           [  945.],\n",
       "           ...,\n",
       "           [  903.],\n",
       "           [  906.],\n",
       "           [  949.]]],\n",
       "\n",
       "\n",
       "         [[[   57.],\n",
       "           [   43.],\n",
       "           [   66.],\n",
       "           ...,\n",
       "           [   74.],\n",
       "           [   73.],\n",
       "           [   73.]],\n",
       "\n",
       "          [[   58.],\n",
       "           [   57.],\n",
       "           [   68.],\n",
       "           ...,\n",
       "           [   68.],\n",
       "           [   61.],\n",
       "           [   62.]],\n",
       "\n",
       "          [[   80.],\n",
       "           [   71.],\n",
       "           [   83.],\n",
       "           ...,\n",
       "           [   72.],\n",
       "           [   66.],\n",
       "           [   71.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[  900.],\n",
       "           [  928.],\n",
       "           [  951.],\n",
       "           ...,\n",
       "           [  874.],\n",
       "           [  899.],\n",
       "           [  940.]],\n",
       "\n",
       "          [[  933.],\n",
       "           [  945.],\n",
       "           [  927.],\n",
       "           ...,\n",
       "           [  900.],\n",
       "           [  934.],\n",
       "           [  949.]],\n",
       "\n",
       "          [[  947.],\n",
       "           [  948.],\n",
       "           [  904.],\n",
       "           ...,\n",
       "           [  905.],\n",
       "           [  913.],\n",
       "           [  939.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[   71.],\n",
       "           [   86.],\n",
       "           [   85.],\n",
       "           ...,\n",
       "           [   69.],\n",
       "           [   63.],\n",
       "           [   49.]],\n",
       "\n",
       "          [[   54.],\n",
       "           [   70.],\n",
       "           [   80.],\n",
       "           ...,\n",
       "           [   66.],\n",
       "           [   63.],\n",
       "           [   58.]],\n",
       "\n",
       "          [[   53.],\n",
       "           [   67.],\n",
       "           [   76.],\n",
       "           ...,\n",
       "           [   64.],\n",
       "           [   63.],\n",
       "           [   60.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1241.],\n",
       "           [ 1345.],\n",
       "           [ 1352.],\n",
       "           ...,\n",
       "           [ 1105.],\n",
       "           [ 1117.],\n",
       "           [ 1133.]],\n",
       "\n",
       "          [[ 1260.],\n",
       "           [ 1348.],\n",
       "           [ 1335.],\n",
       "           ...,\n",
       "           [ 1123.],\n",
       "           [ 1096.],\n",
       "           [ 1108.]],\n",
       "\n",
       "          [[ 1267.],\n",
       "           [ 1344.],\n",
       "           [ 1336.],\n",
       "           ...,\n",
       "           [ 1135.],\n",
       "           [ 1088.],\n",
       "           [ 1100.]]],\n",
       "\n",
       "\n",
       "         [[[   73.],\n",
       "           [   74.],\n",
       "           [   85.],\n",
       "           ...,\n",
       "           [   64.],\n",
       "           [   59.],\n",
       "           [   46.]],\n",
       "\n",
       "          [[   67.],\n",
       "           [   69.],\n",
       "           [   81.],\n",
       "           ...,\n",
       "           [   60.],\n",
       "           [   63.],\n",
       "           [   52.]],\n",
       "\n",
       "          [[   70.],\n",
       "           [   74.],\n",
       "           [   77.],\n",
       "           ...,\n",
       "           [   67.],\n",
       "           [   70.],\n",
       "           [   57.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1213.],\n",
       "           [ 1351.],\n",
       "           [ 1361.],\n",
       "           ...,\n",
       "           [ 1110.],\n",
       "           [ 1126.],\n",
       "           [ 1122.]],\n",
       "\n",
       "          [[ 1242.],\n",
       "           [ 1348.],\n",
       "           [ 1339.],\n",
       "           ...,\n",
       "           [ 1119.],\n",
       "           [ 1104.],\n",
       "           [ 1113.]],\n",
       "\n",
       "          [[ 1222.],\n",
       "           [ 1329.],\n",
       "           [ 1318.],\n",
       "           ...,\n",
       "           [ 1136.],\n",
       "           [ 1094.],\n",
       "           [ 1102.]]],\n",
       "\n",
       "\n",
       "         [[[   74.],\n",
       "           [   68.],\n",
       "           [   67.],\n",
       "           ...,\n",
       "           [   58.],\n",
       "           [   60.],\n",
       "           [   51.]],\n",
       "\n",
       "          [[   78.],\n",
       "           [   76.],\n",
       "           [   73.],\n",
       "           ...,\n",
       "           [   66.],\n",
       "           [   70.],\n",
       "           [   58.]],\n",
       "\n",
       "          [[   85.],\n",
       "           [   82.],\n",
       "           [   72.],\n",
       "           ...,\n",
       "           [   71.],\n",
       "           [   83.],\n",
       "           [   71.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[ 1199.],\n",
       "           [ 1333.],\n",
       "           [ 1356.],\n",
       "           ...,\n",
       "           [ 1082.],\n",
       "           [ 1104.],\n",
       "           [ 1110.]],\n",
       "\n",
       "          [[ 1228.],\n",
       "           [ 1335.],\n",
       "           [ 1345.],\n",
       "           ...,\n",
       "           [ 1094.],\n",
       "           [ 1087.],\n",
       "           [ 1102.]],\n",
       "\n",
       "          [[ 1200.],\n",
       "           [ 1313.],\n",
       "           [ 1322.],\n",
       "           ...,\n",
       "           [ 1113.],\n",
       "           [ 1075.],\n",
       "           [ 1096.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[   42.],\n",
       "           [   37.],\n",
       "           [   59.],\n",
       "           ...,\n",
       "           [   56.],\n",
       "           [   71.],\n",
       "           [   63.]],\n",
       "\n",
       "          [[   67.],\n",
       "           [   45.],\n",
       "           [   52.],\n",
       "           ...,\n",
       "           [   67.],\n",
       "           [   81.],\n",
       "           [   60.]],\n",
       "\n",
       "          [[  102.],\n",
       "           [   75.],\n",
       "           [   65.],\n",
       "           ...,\n",
       "           [  127.],\n",
       "           [  133.],\n",
       "           [  126.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[  120.],\n",
       "           [   73.],\n",
       "           [   87.],\n",
       "           ...,\n",
       "           [  977.],\n",
       "           [  982.],\n",
       "           [  957.]],\n",
       "\n",
       "          [[  109.],\n",
       "           [   76.],\n",
       "           [   99.],\n",
       "           ...,\n",
       "           [  950.],\n",
       "           [  939.],\n",
       "           [  947.]],\n",
       "\n",
       "          [[   76.],\n",
       "           [   76.],\n",
       "           [   86.],\n",
       "           ...,\n",
       "           [  966.],\n",
       "           [  931.],\n",
       "           [  965.]]],\n",
       "\n",
       "\n",
       "         [[[   46.],\n",
       "           [   42.],\n",
       "           [   44.],\n",
       "           ...,\n",
       "           [   43.],\n",
       "           [   78.],\n",
       "           [   60.]],\n",
       "\n",
       "          [[   66.],\n",
       "           [   67.],\n",
       "           [   59.],\n",
       "           ...,\n",
       "           [   92.],\n",
       "           [  105.],\n",
       "           [   89.]],\n",
       "\n",
       "          [[  102.],\n",
       "           [   99.],\n",
       "           [  107.],\n",
       "           ...,\n",
       "           [  189.],\n",
       "           [  183.],\n",
       "           [  180.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[  120.],\n",
       "           [   88.],\n",
       "           [   84.],\n",
       "           ...,\n",
       "           [ 1000.],\n",
       "           [  985.],\n",
       "           [  956.]],\n",
       "\n",
       "          [[  106.],\n",
       "           [   79.],\n",
       "           [   85.],\n",
       "           ...,\n",
       "           [  943.],\n",
       "           [  923.],\n",
       "           [  937.]],\n",
       "\n",
       "          [[   78.],\n",
       "           [   73.],\n",
       "           [   63.],\n",
       "           ...,\n",
       "           [  928.],\n",
       "           [  908.],\n",
       "           [  950.]]],\n",
       "\n",
       "\n",
       "         [[[   44.],\n",
       "           [   73.],\n",
       "           [   63.],\n",
       "           ...,\n",
       "           [   44.],\n",
       "           [   71.],\n",
       "           [   67.]],\n",
       "\n",
       "          [[   48.],\n",
       "           [   66.],\n",
       "           [   76.],\n",
       "           ...,\n",
       "           [  118.],\n",
       "           [  130.],\n",
       "           [  119.]],\n",
       "\n",
       "          [[  129.],\n",
       "           [  115.],\n",
       "           [  145.],\n",
       "           ...,\n",
       "           [  257.],\n",
       "           [  242.],\n",
       "           [  239.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[  114.],\n",
       "           [  100.],\n",
       "           [   78.],\n",
       "           ...,\n",
       "           [ 1022.],\n",
       "           [  983.],\n",
       "           [  935.]],\n",
       "\n",
       "          [[   93.],\n",
       "           [   85.],\n",
       "           [   64.],\n",
       "           ...,\n",
       "           [  956.],\n",
       "           [  921.],\n",
       "           [  921.]],\n",
       "\n",
       "          [[   55.],\n",
       "           [   80.],\n",
       "           [   82.],\n",
       "           ...,\n",
       "           [  914.],\n",
       "           [  904.],\n",
       "           [  937.]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[  265.],\n",
       "           [  285.],\n",
       "           [  283.],\n",
       "           ...,\n",
       "           [  301.],\n",
       "           [  308.],\n",
       "           [  313.]],\n",
       "\n",
       "          [[  249.],\n",
       "           [  191.],\n",
       "           [  167.],\n",
       "           ...,\n",
       "           [  167.],\n",
       "           [  180.],\n",
       "           [  164.]],\n",
       "\n",
       "          [[  125.],\n",
       "           [   70.],\n",
       "           [   36.],\n",
       "           ...,\n",
       "           [  104.],\n",
       "           [  122.],\n",
       "           [  101.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[   93.],\n",
       "           [  108.],\n",
       "           [   88.],\n",
       "           ...,\n",
       "           [ 1058.],\n",
       "           [ 1077.],\n",
       "           [ 1088.]],\n",
       "\n",
       "          [[   83.],\n",
       "           [  104.],\n",
       "           [  107.],\n",
       "           ...,\n",
       "           [ 1081.],\n",
       "           [ 1079.],\n",
       "           [ 1098.]],\n",
       "\n",
       "          [[   86.],\n",
       "           [   89.],\n",
       "           [  115.],\n",
       "           ...,\n",
       "           [ 1112.],\n",
       "           [ 1088.],\n",
       "           [ 1095.]]],\n",
       "\n",
       "\n",
       "         [[[  283.],\n",
       "           [  308.],\n",
       "           [  300.],\n",
       "           ...,\n",
       "           [  350.],\n",
       "           [  326.],\n",
       "           [  322.]],\n",
       "\n",
       "          [[  259.],\n",
       "           [  240.],\n",
       "           [  209.],\n",
       "           ...,\n",
       "           [  210.],\n",
       "           [  204.],\n",
       "           [  184.]],\n",
       "\n",
       "          [[  141.],\n",
       "           [   93.],\n",
       "           [   70.],\n",
       "           ...,\n",
       "           [  130.],\n",
       "           [  141.],\n",
       "           [  125.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[   83.],\n",
       "           [  106.],\n",
       "           [  112.],\n",
       "           ...,\n",
       "           [ 1045.],\n",
       "           [ 1078.],\n",
       "           [ 1081.]],\n",
       "\n",
       "          [[   75.],\n",
       "           [   94.],\n",
       "           [  112.],\n",
       "           ...,\n",
       "           [ 1070.],\n",
       "           [ 1076.],\n",
       "           [ 1081.]],\n",
       "\n",
       "          [[   80.],\n",
       "           [   75.],\n",
       "           [  111.],\n",
       "           ...,\n",
       "           [ 1127.],\n",
       "           [ 1098.],\n",
       "           [ 1098.]]],\n",
       "\n",
       "\n",
       "         [[[  288.],\n",
       "           [  304.],\n",
       "           [  322.],\n",
       "           ...,\n",
       "           [  393.],\n",
       "           [  363.],\n",
       "           [  333.]],\n",
       "\n",
       "          [[  277.],\n",
       "           [  252.],\n",
       "           [  254.],\n",
       "           ...,\n",
       "           [  265.],\n",
       "           [  250.],\n",
       "           [  220.]],\n",
       "\n",
       "          [[  185.],\n",
       "           [  121.],\n",
       "           [  105.],\n",
       "           ...,\n",
       "           [  156.],\n",
       "           [  152.],\n",
       "           [  143.]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[   82.],\n",
       "           [   91.],\n",
       "           [  102.],\n",
       "           ...,\n",
       "           [ 1047.],\n",
       "           [ 1072.],\n",
       "           [ 1083.]],\n",
       "\n",
       "          [[   65.],\n",
       "           [   78.],\n",
       "           [   92.],\n",
       "           ...,\n",
       "           [ 1060.],\n",
       "           [ 1063.],\n",
       "           [ 1071.]],\n",
       "\n",
       "          [[   65.],\n",
       "           [   65.],\n",
       "           [   84.],\n",
       "           ...,\n",
       "           [ 1109.],\n",
       "           [ 1104.],\n",
       "           [ 1092.]]]]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_batch, sampled_batch = next(enumerate(trainloader))\n",
    "image_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
    "image_batch, label_batch = image_batch.cuda(), label_batch.cuda()\n",
    "image_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the model on image_batch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Vanderbilt\\Vanderbilt_2024_Spring\\ML Project\\swinunet\\networks\\vision_transformer.py:50\u001b[0m, in \u001b[0;36mSwinUnet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswin_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Vanderbilt\\Vanderbilt_2024_Spring\\ML Project\\swinunet\\networks\\swin_transformer_unet_skip_expand_decoder_sys.py:738\u001b[0m, in \u001b[0;36mSwinTransformerSys.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 738\u001b[0m     x, x_downsample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_up_features(x,x_downsample)\n\u001b[0;32m    740\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_x4(x)\n",
      "File \u001b[1;32m~\\OneDrive - Vanderbilt\\Vanderbilt_2024_Spring\\ML Project\\swinunet\\networks\\swin_transformer_unet_skip_expand_decoder_sys.py:696\u001b[0m, in \u001b[0;36mSwinTransformerSys.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 696\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mape:\n\u001b[0;32m    698\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsolute_pos_embed\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Vanderbilt\\Vanderbilt_2024_Spring\\ML Project\\swinunet\\networks\\swin_transformer_unet_skip_expand_decoder_sys.py:541\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 541\u001b[0m     B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;66;03m# FIXME look at relaxing size constraints\u001b[39;00m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m H \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Run the model on image_batch\n",
    "outputs = model(image_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bl83iyGHGH08"
   },
   "outputs": [],
   "source": [
    "# Paths to your datasets\n",
    "data_path = '/content/drive/MyDrive/Data/NGKDStorage/images/ULS23_Part1/ULS23/novel_data/ULS23_Radboudumc_Bone/images'\n",
    "annotations_path = '/content/drive/MyDrive/Data/NGKDStorage/annotations/ULS23/novel_data/ULS23_Radboudumc_Bone/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZmXOIkhGJhe"
   },
   "outputs": [],
   "source": [
    "class NiftiDataGenerator(Sequence):\n",
    "    def __init__(self, data_filenames, annotations_filenames, batch_size, data_path, annotations_path, image_size=(256, 256)):\n",
    "        self.data_filenames = data_filenames\n",
    "        self.annotations_filenames = annotations_filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        self.annotations_path = annotations_path\n",
    "        self.image_size = image_size\n",
    "        self.indexes = self._get_indexes()\n",
    "\n",
    "    def _get_indexes(self):\n",
    "        indexes = []\n",
    "        for i, fname in enumerate(self.data_filenames):\n",
    "            nii_img = nib.load(os.path.join(self.data_path, fname))\n",
    "            num_slices = nii_img.shape[2]\n",
    "            for slice_index in range(num_slices):\n",
    "                indexes.append((i, slice_index))\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indexes) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_data = []\n",
    "        batch_annotations = []\n",
    "\n",
    "        for i, slice_index in batch_indexes:\n",
    "            data_filename = self.data_filenames[i]\n",
    "            annotations_filename = self.annotations_filenames[i]\n",
    "\n",
    "            data_nii = nib.load(os.path.join(self.data_path, data_filename))\n",
    "            annotations_nii = nib.load(os.path.join(self.annotations_path, annotations_filename))\n",
    "\n",
    "            data_img = data_nii.get_fdata()[:, :, slice_index]\n",
    "            annotations_img = annotations_nii.get_fdata()[:, :, slice_index]\n",
    "\n",
    "            # Convert grayscale to 3 channels by repeating the slice\n",
    "            data_slice_3ch = np.repeat(data_img[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "            # Resize images (if necessary)\n",
    "            data_resized = resize(data_slice_3ch, self.image_size + (3,), anti_aliasing=True)\n",
    "            annotations_resized = resize(annotations_img, self.image_size, anti_aliasing=True, order=0, preserve_range=True)\n",
    "\n",
    "            # Remove the unwanted dimension safely\n",
    "            if data_resized.shape[-1] == 1:\n",
    "                data_resized = data_resized[..., 0]\n",
    "\n",
    "            batch_data.append(data_resized)\n",
    "            batch_annotations.append(annotations_resized)\n",
    "\n",
    "        return np.array(batch_data), np.array(batch_annotations).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAfwewBTGLUA"
   },
   "outputs": [],
   "source": [
    "# Get the list of filenames for data and annotations\n",
    "data_filenames = [f for f in os.listdir(data_path) if f.endswith('.nii.gz')]\n",
    "annotations_filenames = [f for f in os.listdir(annotations_path) if f.endswith('.nii.gz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEzTImjDGPD5"
   },
   "outputs": [],
   "source": [
    "# Sort the lists to ensure they are aligned\n",
    "data_filenames.sort()\n",
    "annotations_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ogFzagLGQ3U"
   },
   "outputs": [],
   "source": [
    "# Split the filenames into training and validation sets\n",
    "train_data_filenames, val_data_filenames, train_annotations_filenames, val_annotations_filenames = train_test_split(data_filenames, annotations_filenames, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25Arkb3kGaxS"
   },
   "outputs": [],
   "source": [
    "batch_size = 4  # Adjust based on your GPU memory\n",
    "\n",
    "# Create the data generators\n",
    "train_generator = NiftiDataGenerator(\n",
    "    data_filenames=train_data_filenames,\n",
    "    annotations_filenames=train_annotations_filenames,\n",
    "    batch_size=batch_size,\n",
    "    data_path=data_path,\n",
    "    annotations_path=annotations_path\n",
    ")\n",
    "\n",
    "val_generator = NiftiDataGenerator(\n",
    "    data_filenames=val_data_filenames,\n",
    "    annotations_filenames=val_annotations_filenames,\n",
    "    batch_size=batch_size,\n",
    "    data_path=data_path,\n",
    "    annotations_path=annotations_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKXy9y94G17L"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def DeeplabV3Plus(image_size, num_classes):\n",
    "    # Load a pretrained MobileNetV2 model as a base\n",
    "    base_model = MobileNetV2(input_shape=[image_size, image_size, 3], include_top=False)\n",
    "\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "        'block_1_expand_relu',   # 64x64\n",
    "        'block_3_expand_relu',   # 32x32\n",
    "        'block_6_expand_relu',   # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "        'block_16_project',      # 4x4\n",
    "    ]\n",
    "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Create the feature extraction model\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "    down_stack.trainable = False\n",
    "\n",
    "    # Define the upsampling layers\n",
    "    up_stack = [\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 4x4 -> 8x8\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 8x8 -> 16x16\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 16x16 -> 32x32\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 32x32 -> 64x64\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 64x64 -> 128x128\n",
    "        layers.UpSampling2D(size=(2, 2)),  # 128x128 -> 256x256\n",
    "    ]\n",
    "\n",
    "    inputs = tf.keras.Input(shape=[image_size, image_size, 3])\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(x)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = layers.Conv2DTranspose(\n",
    "        num_classes, 3, strides=2,\n",
    "        padding='same', activation='softmax')  # Use strides=2 to match the original image size if needed\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLP1CMGyKsNt"
   },
   "outputs": [],
   "source": [
    "# Define the size of your images and number of classes\n",
    "image_size = 256  # Size of your input images\n",
    "num_classes = 2   # Two classes: background and lesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avU9W_OHMmMG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    intersection = tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.cast(y_pred, tf.float32))\n",
    "    union = tf.reduce_sum(tf.cast(y_true, tf.float32)) + tf.reduce_sum(tf.cast(y_pred, tf.float32))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice\n",
    "\n",
    "# Custom callback to print the Dice score after each epoch\n",
    "class PrintDiceScoreCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}: Dice Score is {logs['dice_score']:.4f} for training and {logs['val_dice_score']:.4f} for validation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1984,
     "status": "ok",
     "timestamp": 1708329203507,
     "user": {
      "displayName": "NGKD",
      "userId": "00773922209354261622"
     },
     "user_tz": 360
    },
    "id": "h3jgDl-4Ktpg",
    "outputId": "bf5cb564-b1e7-42eb-a2f1-64799b098a35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " model_26 (Functional)       [(None, 128, 128, 96),       1841984   ['input_28[0][0]']            \n",
      "                              (None, 64, 64, 144),                                                \n",
      "                              (None, 32, 32, 192),                                                \n",
      "                              (None, 16, 16, 576),                                                \n",
      "                              (None, 8, 8, 320)]                                                  \n",
      "                                                                                                  \n",
      " up_sampling2d_78 (UpSampli  (None, 16, 16, 320)          0         ['model_26[0][4]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_52 (Concatenat  (None, 16, 16, 896)          0         ['up_sampling2d_78[0][0]',    \n",
      " e)                                                                  'model_26[0][3]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_79 (UpSampli  (None, 32, 32, 896)          0         ['concatenate_52[0][0]']      \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_53 (Concatenat  (None, 32, 32, 1088)         0         ['up_sampling2d_79[0][0]',    \n",
      " e)                                                                  'model_26[0][2]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_80 (UpSampli  (None, 64, 64, 1088)         0         ['concatenate_53[0][0]']      \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_54 (Concatenat  (None, 64, 64, 1232)         0         ['up_sampling2d_80[0][0]',    \n",
      " e)                                                                  'model_26[0][1]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_81 (UpSampli  (None, 128, 128, 1232)       0         ['concatenate_54[0][0]']      \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_55 (Concatenat  (None, 128, 128, 1328)       0         ['up_sampling2d_81[0][0]',    \n",
      " e)                                                                  'model_26[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_13 (Conv2  (None, 256, 256, 2)          23906     ['concatenate_55[0][0]']      \n",
      " DTranspose)                                                                                      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1865890 (7.12 MB)\n",
      "Trainable params: 23906 (93.38 KB)\n",
      "Non-trainable params: 1841984 (7.03 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = DeeplabV3Plus(image_size, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[dice_score])\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuHvMbXyMpI6"
   },
   "outputs": [],
   "source": [
    "# Define, compile, and train the model\n",
    "def train_model():\n",
    "    model = None  # Declare model variable first\n",
    "    # Check if a model already exists\n",
    "    model_path = './my_deeplab_model.h5'\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing model.\")\n",
    "        model = tf.keras.models.load_model(model_path, custom_objects={'dice_score': dice_score})\n",
    "    else:\n",
    "        print(\"Creating a new model.\")\n",
    "        model = DeeplabV3Plus(image_size, num_classes)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=[dice_score])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        validation_steps=len(val_generator),\n",
    "        callbacks=[PrintDiceScoreCallback()]\n",
    "    )\n",
    "\n",
    "    # Save the model after training\n",
    "    model.save(model_path)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzdiWDBXMe_9",
    "outputId": "15e262e3-20ca-4fe8-c6df-adb4e69403ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new model.\n",
      " 2264/19040 [==>...........................] - ETA: 10:08:20 - loss: 0.0110 - dice_score: 0.8225"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = train_model()\n",
    "\n",
    "# Plot the Dice score\n",
    "plt.plot(history.history['dice_score'], label='Training Dice Score')\n",
    "plt.plot(history.history['val_dice_score'], label='Validation Dice Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
